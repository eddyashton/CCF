/* 
  This file was generated by KreMLin <https://github.com/FStarLang/kremlin>
  KreMLin invocation: /mnt/e/everest/verify/kremlin/krml -skip-compilation -no-prefix MerkleTree.New.Low -no-prefix MerkleTree.New.Low.Serialization -add-include "kremlin/internal/compat.h" -bundle Prims,FStar.*,LowStar.*,C,C.*[rename=Merkle_Kremlib] -bundle MerkleTree.New.Low= -bundle MerkleTree.New.Low.Serialization= -bundle *[rename=Merkle_EverCrypt] -library EverCrypt,EverCrypt.* -tmpdir dist/compact/ -ccopt -O3 -ccopt -march=native -ccopt -mtune=native -ccopt -funroll-loops -warn-error +9 -fparentheses -fcurly-braces -fno-shadow -ftail-calls -o libmerkletree.a .output/prims.krml .output/FStar_Pervasives_Native.krml .output/FStar_Pervasives.krml .output/FStar_Mul.krml .output/FStar_Squash.krml .output/FStar_Classical.krml .output/FStar_StrongExcludedMiddle.krml .output/FStar_FunctionalExtensionality.krml .output/FStar_List_Tot_Base.krml .output/FStar_List_Tot_Properties.krml .output/FStar_List_Tot.krml .output/FStar_Seq_Base.krml .output/FStar_Seq_Properties.krml .output/FStar_Seq.krml .output/FStar_Math_Lib.krml .output/FStar_Math_Lemmas.krml .output/FStar_BitVector.krml .output/FStar_UInt.krml .output/FStar_UInt32.krml .output/FStar_Int.krml .output/FStar_Int16.krml .output/Opaque_s.krml .output/Collections_Seqs_s.krml .output/FStar_UInt8.krml .output/Words_s.krml .output/Words_Four_s.krml .output/Words_Two_s.krml .output/Words_Seq_s.krml .output/Types_s.krml .output/TypesNative_s.krml .output/Arch_TypesNative.krml .output/Prop_s.krml .output/Util_Meta.krml .output/FStar_Set.krml .output/FStar_Map.krml .output/X64_Machine_s.krml .output/X64_CPU_Features_s.krml .output/Map16.krml .output/X64_Vale_Xmms.krml .output/X64_Vale_Regs.krml .output/FStar_Float.krml .output/FStar_UInt64.krml .output/FStar_Exn.krml .output/FStar_Preorder.krml .output/FStar_Monotonic_Witnessed.krml .output/FStar_Ghost.krml .output/FStar_ErasedLogic.krml .output/FStar_PropositionalExtensionality.krml .output/FStar_PredicateExtensionality.krml .output/FStar_TSet.krml .output/FStar_Monotonic_Heap.krml .output/FStar_Heap.krml .output/FStar_ST.krml .output/FStar_All.krml .output/FStar_IO.krml .output/Words_Two.krml .output/Collections_Seqs.krml .output/Words_Seq.krml .output/Arch_Types.krml .output/AES_s.krml .output/Math_Poly2_Defs_s.krml .output/Math_Poly2_s.krml .output/Math_Poly2_Bits_s.krml .output/FStar_Monotonic_HyperHeap.krml .output/FStar_Monotonic_HyperStack.krml .output/FStar_HyperStack.krml .output/FStar_HyperStack_ST.krml .output/FStar_HyperStack_All.krml .output/FStar_Kremlin_Endianness.krml .output/FStar_Int64.krml .output/FStar_Int63.krml .output/FStar_Int32.krml .output/FStar_Int8.krml .output/FStar_UInt63.krml .output/FStar_UInt16.krml .output/FStar_Int_Cast.krml .output/FStar_UInt128.krml .output/Spec_Hash_Definitions.krml .output/Spec_Hash_Lemmas0.krml .output/Spec_Hash_PadFinish.krml .output/Spec_Loops.krml .output/FStar_List.krml .output/Spec_SHA2_Constants.krml .output/Spec_SHA2.krml .output/X64_CryptoInstructions_s.krml .output/X64_Bytes_Semantics_s.krml .output/FStar_Char.krml .output/FStar_BaseTypes.krml .output/X64_Taint_Semantics_s.krml .output/X64_Bytes_Semantics.krml .output/FStar_Universe.krml .output/FStar_GSet.krml .output/FStar_ModifiesGen.krml .output/FStar_Range.krml .output/FStar_Reflection_Types.krml .output/FStar_Tactics_Types.krml .output/FStar_Tactics_Result.krml .output/FStar_Tactics_Effect.krml .output/FStar_Tactics_Util.krml .output/FStar_Reflection_Data.krml .output/FStar_Reflection_Const.krml .output/FStar_String.krml .output/FStar_Order.krml .output/FStar_Reflection_Basic.krml .output/FStar_Reflection_Derived.krml .output/FStar_Tactics_Builtins.krml .output/FStar_Reflection_Formula.krml .output/FStar_Reflection_Derived_Lemmas.krml .output/FStar_Reflection.krml .output/FStar_Tactics_Derived.krml .output/FStar_Tactics_Logic.krml .output/FStar_Tactics.krml .output/FStar_BigOps.krml .output/LowStar_Monotonic_Buffer.krml .output/LowStar_BufferView.krml .output/Views.krml .output/LowStar_Buffer.krml .output/LowStar_Modifies.krml .output/LowStar_ModifiesPat.krml .output/Interop.krml .output/X64_Memory.krml .output/BufferViewHelpers.krml .output/X64_BufferViewStore.krml .output/X64_Memory_Sems.krml .output/X64_Vale_State.krml .output/X64_Vale_StateLemmas.krml .output/X64_Vale_Lemmas.krml .output/X64_Print_s.krml .output/X64_Vale_Decls.krml .output/X64_Vale_QuickCode.krml .output/X64_Vale_QuickCodes.krml .output/X64_Taint_Semantics.krml .output/X64_Vale_InsLemmas.krml .output/X64_Vale_InsBasic.krml .output/X64_Cpuid.krml .output/X64_Cpuidstdcall.krml .output/Vale_check_aesni_stdcall.krml .output/Spec_SHA1.krml .output/Spec_MD5.krml .output/Spec_Hash.krml .output/Spec_Hash_Incremental.krml .output/Spec_Hash_Lemmas.krml .output/LowStar_BufferOps.krml .output/C_Loops.krml .output/C_Endianness.krml .output/Hacl_Hash_Lemmas.krml .output/Hacl_Hash_Definitions.krml .output/FStar_Int_Cast_Full.krml .output/Hacl_Hash_PadFinish.krml .output/Hacl_Hash_MD.krml .output/X64_Vale_InsMem.krml .output/X64_Vale_InsVector.krml .output/X64_Stack.krml .output/Workarounds.krml .output/SHA_helpers.krml .output/X64_Vale_InsSha.krml .output/X64_SHA.krml .output/Vale_sha_update_bytes_stdcall.krml .output/Interop_assumptions.krml .output/Sha_update_bytes_stdcall.krml .output/LowStar_ImmutableBuffer.krml .output/Hacl_Hash_Core_SHA2_Constants.krml .output/Hacl_Hash_Core_SHA2.krml .output/Hacl_Hash_SHA2.krml .output/Hacl_Hash_Core_SHA1.krml .output/Hacl_Hash_SHA1.krml .output/Hacl_Hash_Core_MD5.krml .output/Hacl_Hash_MD5.krml .output/C.krml .output/C_String.krml .output/C_Failure.krml .output/FStar_Int128.krml .output/FStar_Int31.krml .output/FStar_UInt31.krml .output/FStar_Integers.krml .output/EverCrypt_StaticConfig.krml .output/Vale_check_sha_stdcall.krml .output/Check_sha_stdcall.krml .output/Check_aesni_stdcall.krml .output/EverCrypt_AutoConfig2.krml .output/EverCrypt_Helpers.krml .output/EverCrypt_Hash.krml .output/LowStar_Vector.krml .output/LowStar_Regional.krml .output/LowStar_RVector.krml .output/FStar_Dyn.krml .output/EverCrypt_Vale.krml .output/EverCrypt_Specs.krml .output/EverCrypt_OpenSSL.krml .output/EverCrypt_Hacl.krml .output/EverCrypt_BCrypt.krml .output/EverCrypt.krml .output/MerkleTree_Spec.krml .output/MerkleTree_New_High.krml .output/LowStar_Regional_Instances.krml .output/MerkleTree_New_Low.krml .output/MerkleTree_New_Low_Serialization.krml
  F* version: d37e991b
  KreMLin version: d8ba3898
 */

#include "MerkleTree_New_Low.h"

uint32_t hash_size = (uint32_t)32U;

uint8_t *hash_r_alloc()
{
  KRML_CHECK_SIZE(sizeof (uint8_t), hash_size);
  uint8_t *buf = KRML_HOST_CALLOC(hash_size, sizeof (uint8_t));
  return buf;
}

void hash_r_free(uint8_t *v1)
{
  KRML_HOST_FREE(v1);
}

void hash_copy(uint8_t *src, uint8_t *dst)
{
  memcpy(dst, src, hash_size * sizeof src[0U]);
}

typedef struct LowStar_RVector_copyable__uint8_t__s { void (*copy)(uint8_t *x0, uint8_t *x1); }
LowStar_RVector_copyable__uint8_t_;

static LowStar_RVector_copyable__uint8_t_ hcpy = { .copy = hash_copy };

LowStar_Vector_vector_str__uint8_t_
hash_vec_dummy = { .sz = (uint32_t)0U, .cap = (uint32_t)0U, .vs = NULL };

static LowStar_Vector_vector_str__uint8_t_
LowStar_Vector_alloc_reserve__uint8_t_(uint32_t len1, uint8_t *ia)
{
  KRML_CHECK_SIZE(sizeof (uint8_t *), len1);
  uint8_t **buf = KRML_HOST_MALLOC(sizeof (uint8_t *) * len1);
  for (uint32_t _i = 0U; _i < len1; ++_i)
    buf[_i] = ia;
  return ((LowStar_Vector_vector_str__uint8_t_){ .sz = (uint32_t)0U, .cap = len1, .vs = buf });
}

LowStar_Vector_vector_str__uint8_t_ hash_vec_r_alloc()
{
  LowStar_Regional_regional__uint8_t_
  x0 = { .dummy = NULL, .r_alloc = hash_r_alloc, .r_free = hash_r_free };
  uint8_t *ia1 = x0.dummy;
  return LowStar_Vector_alloc_reserve__uint8_t_((uint32_t)1U, ia1);
}

uint8_t *LowStar_Vector_index__uint8_t_(LowStar_Vector_vector_str__uint8_t_ vec, uint32_t i)
{
  return vec.vs[i];
}

static void
LowStar_RVector_free_elems__uint8_t_(
  LowStar_Regional_regional__uint8_t_ rg0,
  LowStar_Vector_vector_str__uint8_t_ rv0,
  uint32_t idx0
)
{
  LowStar_Regional_regional__uint8_t_ rg = rg0;
  LowStar_Vector_vector_str__uint8_t_ rv = rv0;
  uint32_t idx = idx0;
  while (true)
  {
    uint8_t *uu____0 = LowStar_Vector_index__uint8_t_(rv, idx);
    rg.r_free(uu____0);
    if (idx != (uint32_t)0U)
    {
      idx = idx - (uint32_t)1U;
    }
    else
    {
      return;
    }
  }
  KRML_HOST_PRINTF("KreMLin abort at %s:%d\n%s\n",
    __FILE__,
    __LINE__,
    "unreachable, returns inserted above");
  KRML_HOST_EXIT(255U);
}

static void LowStar_Vector_free__uint8_t_(LowStar_Vector_vector_str__uint8_t_ vec)
{
  KRML_HOST_FREE(vec.vs);
}

static void
LowStar_RVector_free__uint8_t_(
  LowStar_Regional_regional__uint8_t_ rg,
  LowStar_Vector_vector_str__uint8_t_ rv
)
{
  if (!(rv.sz == (uint32_t)0U))
  {
    LowStar_RVector_free_elems__uint8_t_(rg, rv, rv.sz - (uint32_t)1U);
  }
  LowStar_Vector_free__uint8_t_(rv);
}

void hash_vec_r_free(LowStar_Vector_vector_str__uint8_t_ v1)
{
  LowStar_RVector_free__uint8_t_((
      (LowStar_Regional_regional__uint8_t_){
        .dummy = NULL,
        .r_alloc = hash_r_alloc,
        .r_free = hash_r_free
      }
    ),
    v1);
}

uint8_t *(*init_hash)() = hash_r_alloc;

void (*free_hash)(uint8_t *x0) = hash_r_free;

void hash_2(uint8_t *src1, uint8_t *src2, uint8_t *dst)
{
  uint8_t cb[64U] = { 0U };
  memcpy(cb, src1, hash_size * sizeof src1[0U]);
  memcpy(cb + (uint32_t)32U, src2, hash_size * sizeof src2[0U]);
  uint32_t buf[8U] = { 0U };
  EverCrypt_Hash_state_s s = { .tag = EverCrypt_Hash_SHA2_256_s, { .case_SHA2_256_s = buf } };
  EverCrypt_Hash_state_s st = s;
  EverCrypt_Hash_init(&st);
  EverCrypt_Hash_update(&st, cb);
  EverCrypt_Hash_finish(&st, dst);
}

uint32_t uint32_32_max = (uint32_t)4294967295U;

uint64_t uint32_max = (uint64_t)4294967295U;

uint64_t uint64_max = (uint64_t)18446744073709551615U;

uint64_t offset_range_limit = (uint64_t)4294967295U;

uint32_t merkle_tree_size_lg = (uint32_t)32U;

bool uu___is_MT(merkle_tree projectee)
{
  return true;
}

uint64_t __proj__MT__item__offset(merkle_tree projectee)
{
  return projectee.offset;
}

uint32_t __proj__MT__item__i(merkle_tree projectee)
{
  return projectee.i;
}

uint32_t __proj__MT__item__j(merkle_tree projectee)
{
  return projectee.j;
}

LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_
__proj__MT__item__hs(merkle_tree projectee)
{
  return projectee.hs;
}

bool __proj__MT__item__rhs_ok(merkle_tree projectee)
{
  return projectee.rhs_ok;
}

LowStar_Vector_vector_str__uint8_t_ __proj__MT__item__rhs(merkle_tree projectee)
{
  return projectee.rhs;
}

uint8_t *__proj__MT__item__mroot(merkle_tree projectee)
{
  return projectee.mroot;
}

bool
merkle_tree_conditions(
  uint64_t offset1,
  uint32_t i1,
  uint32_t j,
  LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_ hs,
  bool rhs_ok,
  LowStar_Vector_vector_str__uint8_t_ rhs,
  uint8_t *mroot
)
{
  return
    j
    >= i1
    && uint64_max - offset1 >= (uint64_t)j
    && hs.sz == (uint32_t)32U
    && rhs.sz == (uint32_t)32U;
}

uint32_t offset_of(uint32_t i1)
{
  if (i1 % (uint32_t)2U == (uint32_t)0U)
  {
    return i1;
  }
  else
  {
    return i1 - (uint32_t)1U;
  }
}

LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_
LowStar_Vector_alloc_rid__LowStar_Vector_vector_str_uint8_t_(
  uint32_t len1,
  LowStar_Vector_vector_str__uint8_t_ v1
)
{
  KRML_CHECK_SIZE(sizeof (LowStar_Vector_vector_str__uint8_t_), len1);
  LowStar_Vector_vector_str__uint8_t_
  *buf = KRML_HOST_MALLOC(sizeof (LowStar_Vector_vector_str__uint8_t_) * len1);
  for (uint32_t _i = 0U; _i < len1; ++_i)
    buf[_i] = v1;
  return
    (
      (LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_){
        .sz = len1,
        .cap = len1,
        .vs = buf
      }
    );
}

void
LowStar_Vector_assign__LowStar_Vector_vector_str_uint8_t_(
  LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_ vec,
  uint32_t i,
  LowStar_Vector_vector_str__uint8_t_ v1
)
{
  (vec.vs + i)[0U] = v1;
}

static void
LowStar_RVector_alloc___LowStar_Vector_vector_str_uint8_t_(
  LowStar_Regional_regional__LowStar_Vector_vector_str__uint8_t_ rg,
  LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_ rv,
  uint32_t cidx
)
{
  if (!(cidx == (uint32_t)0U))
  {
    LowStar_Vector_vector_str__uint8_t_ v1 = rg.r_alloc();
    LowStar_Vector_assign__LowStar_Vector_vector_str_uint8_t_(rv, cidx - (uint32_t)1U, v1);
    LowStar_RVector_alloc___LowStar_Vector_vector_str_uint8_t_(rg, rv, cidx - (uint32_t)1U);
  }
}

static LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_
LowStar_RVector_alloc_rid__LowStar_Vector_vector_str_uint8_t_(
  LowStar_Regional_regional__LowStar_Vector_vector_str__uint8_t_ rg,
  uint32_t len1
)
{
  LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_
  vec = LowStar_Vector_alloc_rid__LowStar_Vector_vector_str_uint8_t_(len1, rg.dummy);
  LowStar_RVector_alloc___LowStar_Vector_vector_str_uint8_t_(rg, vec, len1);
  return vec;
}

LowStar_Vector_vector_str__uint8_t_
LowStar_Vector_alloc_rid__uint8_t_(uint32_t len1, uint8_t *v1)
{
  KRML_CHECK_SIZE(sizeof (uint8_t *), len1);
  uint8_t **buf = KRML_HOST_MALLOC(sizeof (uint8_t *) * len1);
  for (uint32_t _i = 0U; _i < len1; ++_i)
    buf[_i] = v1;
  return ((LowStar_Vector_vector_str__uint8_t_){ .sz = len1, .cap = len1, .vs = buf });
}

void
LowStar_Vector_assign__uint8_t_(
  LowStar_Vector_vector_str__uint8_t_ vec,
  uint32_t i,
  uint8_t *v1
)
{
  (vec.vs + i)[0U] = v1;
}

static void
LowStar_RVector_alloc___uint8_t_(
  LowStar_Regional_regional__uint8_t_ rg,
  LowStar_Vector_vector_str__uint8_t_ rv,
  uint32_t cidx
)
{
  if (!(cidx == (uint32_t)0U))
  {
    uint8_t *v1 = rg.r_alloc();
    LowStar_Vector_assign__uint8_t_(rv, cidx - (uint32_t)1U, v1);
    LowStar_RVector_alloc___uint8_t_(rg, rv, cidx - (uint32_t)1U);
  }
}

static LowStar_Vector_vector_str__uint8_t_
LowStar_RVector_alloc_rid__uint8_t_(LowStar_Regional_regional__uint8_t_ rg, uint32_t len1)
{
  LowStar_Vector_vector_str__uint8_t_ vec = LowStar_Vector_alloc_rid__uint8_t_(len1, rg.dummy);
  LowStar_RVector_alloc___uint8_t_(rg, vec, len1);
  return vec;
}

static merkle_tree *create_empty_mt()
{
  LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_
  hs =
    LowStar_RVector_alloc_rid__LowStar_Vector_vector_str_uint8_t_((
        (LowStar_Regional_regional__LowStar_Vector_vector_str__uint8_t_){
          .dummy = hash_vec_dummy,
          .r_alloc = hash_vec_r_alloc,
          .r_free = hash_vec_r_free
        }
      ),
      (uint32_t)32U);
  LowStar_Vector_vector_str__uint8_t_
  rhs =
    LowStar_RVector_alloc_rid__uint8_t_((
        (LowStar_Regional_regional__uint8_t_){
          .dummy = NULL,
          .r_alloc = hash_r_alloc,
          .r_free = hash_r_free
        }
      ),
      (uint32_t)32U);
  LowStar_Regional_regional__uint8_t_
  x0 = { .dummy = NULL, .r_alloc = hash_r_alloc, .r_free = hash_r_free };
  uint8_t *mroot = x0.r_alloc();
  KRML_CHECK_SIZE(sizeof (merkle_tree), (uint32_t)1U);
  merkle_tree *mt = KRML_HOST_MALLOC(sizeof (merkle_tree));
  mt[0U]
  =
    (
      (merkle_tree){
        .offset = (uint64_t)0U,
        .i = (uint32_t)0U,
        .j = (uint32_t)0U,
        .hs = hs,
        .rhs_ok = false,
        .rhs = rhs,
        .mroot = mroot
      }
    );
  return mt;
}

LowStar_Vector_vector_str__uint8_t_
LowStar_Vector_index__LowStar_Vector_vector_str_uint8_t_(
  LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_ vec,
  uint32_t i
)
{
  return vec.vs[i];
}

static void
LowStar_RVector_free_elems__LowStar_Vector_vector_str_uint8_t_(
  LowStar_Regional_regional__LowStar_Vector_vector_str__uint8_t_ rg0,
  LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_ rv0,
  uint32_t idx0
)
{
  LowStar_Regional_regional__LowStar_Vector_vector_str__uint8_t_ rg = rg0;
  LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_ rv = rv0;
  uint32_t idx = idx0;
  while (true)
  {
    LowStar_Vector_vector_str__uint8_t_
    uu____0 = LowStar_Vector_index__LowStar_Vector_vector_str_uint8_t_(rv, idx);
    rg.r_free(uu____0);
    if (idx != (uint32_t)0U)
    {
      idx = idx - (uint32_t)1U;
    }
    else
    {
      return;
    }
  }
  KRML_HOST_PRINTF("KreMLin abort at %s:%d\n%s\n",
    __FILE__,
    __LINE__,
    "unreachable, returns inserted above");
  KRML_HOST_EXIT(255U);
}

static void
LowStar_Vector_free__LowStar_Vector_vector_str_uint8_t_(
  LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_ vec
)
{
  KRML_HOST_FREE(vec.vs);
}

static void
LowStar_RVector_free__LowStar_Vector_vector_str_uint8_t_(
  LowStar_Regional_regional__LowStar_Vector_vector_str__uint8_t_ rg,
  LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_ rv
)
{
  if (!(rv.sz == (uint32_t)0U))
  {
    LowStar_RVector_free_elems__LowStar_Vector_vector_str_uint8_t_(rg, rv, rv.sz - (uint32_t)1U);
  }
  LowStar_Vector_free__LowStar_Vector_vector_str_uint8_t_(rv);
}

void mt_free(merkle_tree *mt)
{
  merkle_tree mtv = *mt;
  LowStar_RVector_free__LowStar_Vector_vector_str_uint8_t_((
      (LowStar_Regional_regional__LowStar_Vector_vector_str__uint8_t_){
        .dummy = hash_vec_dummy,
        .r_alloc = hash_vec_r_alloc,
        .r_free = hash_vec_r_free
      }
    ),
    mtv.hs);
  LowStar_RVector_free__uint8_t_((
      (LowStar_Regional_regional__uint8_t_){
        .dummy = NULL,
        .r_alloc = hash_r_alloc,
        .r_free = hash_r_free
      }
    ),
    mtv.rhs);
  LowStar_Regional_regional__uint8_t_
  x0 = { .dummy = NULL, .r_alloc = hash_r_alloc, .r_free = hash_r_free };
  x0.r_free(mtv.mroot);
  KRML_HOST_FREE(mt);
}

static LowStar_Vector_vector_str__uint8_t_
LowStar_Vector_insert__uint8_t_(LowStar_Vector_vector_str__uint8_t_ vec, uint8_t *v1)
{
  uint32_t sz = vec.sz;
  uint32_t cap = vec.cap;
  uint8_t **vs = vec.vs;
  if (sz == cap)
  {
    uint32_t ncap = LowStar_Vector_new_capacity(cap);
    KRML_CHECK_SIZE(sizeof (uint8_t *), ncap);
    uint8_t **nvs = KRML_HOST_MALLOC(sizeof (uint8_t *) * ncap);
    for (uint32_t _i = 0U; _i < ncap; ++_i)
      nvs[_i] = v1;
    memcpy(nvs, vs, sz * sizeof vs[0U]);
    nvs[sz] = v1;
    KRML_HOST_FREE(vs);
    return
      ((LowStar_Vector_vector_str__uint8_t_){ .sz = sz + (uint32_t)1U, .cap = ncap, .vs = nvs });
  }
  else
  {
    vs[sz] = v1;
    return
      ((LowStar_Vector_vector_str__uint8_t_){ .sz = sz + (uint32_t)1U, .cap = cap, .vs = vs });
  }
}

static LowStar_Vector_vector_str__uint8_t_
LowStar_RVector_insert__uint8_t_(
  LowStar_Regional_regional__uint8_t_ rg,
  LowStar_Vector_vector_str__uint8_t_ rv,
  uint8_t *v1
)
{
  LowStar_Vector_vector_str__uint8_t_ irv = LowStar_Vector_insert__uint8_t_(rv, v1);
  return irv;
}

static LowStar_Vector_vector_str__uint8_t_
LowStar_RVector_insert_copy__uint8_t_(
  LowStar_Regional_regional__uint8_t_ rg,
  LowStar_RVector_copyable__uint8_t_ cp,
  LowStar_Vector_vector_str__uint8_t_ rv,
  uint8_t *v1
)
{
  uint8_t *nv = rg.r_alloc();
  void (*copy)(uint8_t *x0, uint8_t *x1) = cp.copy;
  copy(v1, nv);
  return LowStar_RVector_insert__uint8_t_(rg, rv, nv);
}

static void
LowStar_RVector_assign__LowStar_Vector_vector_str_uint8_t_(
  LowStar_Regional_regional__LowStar_Vector_vector_str__uint8_t_ rg,
  LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_ rv,
  uint32_t i,
  LowStar_Vector_vector_str__uint8_t_ v1
)
{
  LowStar_Vector_assign__LowStar_Vector_vector_str_uint8_t_(rv, i, v1);
}

static void
insert_(
  uint32_t lv,
  uint32_t j,
  LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_ hs,
  uint8_t *acc
)
{
  LowStar_Vector_vector_str__uint8_t_
  uu____0 = LowStar_Vector_index__LowStar_Vector_vector_str_uint8_t_(hs, lv);
  LowStar_Vector_vector_str__uint8_t_
  ihv =
    LowStar_RVector_insert_copy__uint8_t_((
        (LowStar_Regional_regional__uint8_t_){
          .dummy = NULL,
          .r_alloc = hash_r_alloc,
          .r_free = hash_r_free
        }
      ),
      hcpy,
      uu____0,
      acc);
  LowStar_RVector_assign__LowStar_Vector_vector_str_uint8_t_((
      (LowStar_Regional_regional__LowStar_Vector_vector_str__uint8_t_){
        .dummy = hash_vec_dummy,
        .r_alloc = hash_vec_r_alloc,
        .r_free = hash_vec_r_free
      }
    ),
    hs,
    lv,
    ihv);
  if (j % (uint32_t)2U == (uint32_t)1U)
  {
    LowStar_Vector_vector_str__uint8_t_
    lvhs = LowStar_Vector_index__LowStar_Vector_vector_str_uint8_t_(hs, lv);
    hash_2(LowStar_Vector_index__uint8_t_(lvhs, lvhs.sz - (uint32_t)2U), acc, acc);
    insert_(lv + (uint32_t)1U, j / (uint32_t)2U, hs, acc);
  }
}

bool mt_insert_pre(merkle_tree *mt, uint8_t *v1)
{
  merkle_tree uu____0 = *mt;
  return
    uu____0.j
    < uint32_32_max
    && uint64_max - uu____0.offset >= (uint64_t)(uu____0.j + (uint32_t)1U);
}

void mt_insert(merkle_tree *mt, uint8_t *v1)
{
  merkle_tree mtv = *mt;
  LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_ hs = mtv.hs;
  insert_((uint32_t)0U, mtv.j, hs, v1);
  *mt
  =
    (
      (merkle_tree){
        .offset = mtv.offset,
        .i = mtv.i,
        .j = mtv.j + (uint32_t)1U,
        .hs = mtv.hs,
        .rhs_ok = false,
        .rhs = mtv.rhs,
        .mroot = mtv.mroot
      }
    );
}

merkle_tree *mt_create(uint8_t *init1)
{
  merkle_tree *mt = create_empty_mt();
  mt_insert(mt, init1);
  return mt;
}

LowStar_Vector_vector_str__uint8_t_ *init_path()
{
  KRML_CHECK_SIZE(sizeof (LowStar_Vector_vector_str__uint8_t_), (uint32_t)1U);
  LowStar_Vector_vector_str__uint8_t_
  *buf = KRML_HOST_MALLOC(sizeof (LowStar_Vector_vector_str__uint8_t_));
  buf[0U] = hash_vec_r_alloc();
  return buf;
}

static LowStar_Vector_vector_str__uint8_t_
LowStar_Vector_clear__uint8_t_(LowStar_Vector_vector_str__uint8_t_ vec)
{
  return
    ((LowStar_Vector_vector_str__uint8_t_){ .sz = (uint32_t)0U, .cap = vec.cap, .vs = vec.vs });
}

void clear_path(LowStar_Vector_vector_str__uint8_t_ *p1)
{
  *p1 = LowStar_Vector_clear__uint8_t_(*p1);
}

void free_path(LowStar_Vector_vector_str__uint8_t_ *p1)
{
  LowStar_Vector_free__uint8_t_(*p1);
  KRML_HOST_FREE(p1);
}

static void
LowStar_RVector_assign_copy__uint8_t_(
  LowStar_Regional_regional__uint8_t_ rg,
  LowStar_RVector_copyable__uint8_t_ cp,
  LowStar_Vector_vector_str__uint8_t_ rv,
  uint32_t i,
  uint8_t *v1
)
{
  void (*copy)(uint8_t *x0, uint8_t *x1) = cp.copy;
  copy(v1, LowStar_Vector_index__uint8_t_(rv, i));
}

static void
construct_rhs(
  uint32_t lv,
  LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_ hs,
  LowStar_Vector_vector_str__uint8_t_ rhs,
  uint32_t i1,
  uint32_t j,
  uint8_t *acc,
  bool actd
)
{
  uint32_t ofs = offset_of(i1);
  LowStar_Regional_regional__uint8_t_
  x0 = { .dummy = NULL, .r_alloc = hash_r_alloc, .r_free = hash_r_free };
  void (*copy1)(uint8_t *x0, uint8_t *x1) = hcpy.copy;
  if (!(j == (uint32_t)0U))
  {
    if (j % (uint32_t)2U == (uint32_t)0U)
    {
      construct_rhs(lv + (uint32_t)1U, hs, rhs, i1 / (uint32_t)2U, j / (uint32_t)2U, acc, actd);
    }
    else
    {
      if (actd)
      {
        LowStar_RVector_assign_copy__uint8_t_((
            (LowStar_Regional_regional__uint8_t_){
              .dummy = NULL,
              .r_alloc = hash_r_alloc,
              .r_free = hash_r_free
            }
          ),
          hcpy,
          rhs,
          lv,
          acc);
        hash_2(LowStar_Vector_index__uint8_t_(LowStar_Vector_index__LowStar_Vector_vector_str_uint8_t_(hs,
              lv),
            j - (uint32_t)1U - ofs),
          acc,
          acc);
      }
      else
      {
        copy1(LowStar_Vector_index__uint8_t_(LowStar_Vector_index__LowStar_Vector_vector_str_uint8_t_(hs,
              lv),
            j - (uint32_t)1U - ofs),
          acc);
      }
      construct_rhs(lv + (uint32_t)1U, hs, rhs, i1 / (uint32_t)2U, j / (uint32_t)2U, acc, true);
    }
  }
}

bool mt_get_root_pre(merkle_tree *mt, uint8_t *rt)
{
  merkle_tree uu____0 = *mt;
  return true;
}

void mt_get_root(merkle_tree *mt, uint8_t *rt)
{
  merkle_tree mtv = *mt;
  uint64_t prefix = mtv.offset;
  uint32_t i1 = mtv.i;
  uint32_t j = mtv.j;
  LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_ hs = mtv.hs;
  LowStar_Vector_vector_str__uint8_t_ rhs = mtv.rhs;
  uint8_t *mroot = mtv.mroot;
  if (mtv.rhs_ok)
  {
    LowStar_Regional_regional__uint8_t_
    x0 = { .dummy = NULL, .r_alloc = hash_r_alloc, .r_free = hash_r_free };
    hcpy.copy(mroot, rt);
  }
  else
  {
    construct_rhs((uint32_t)0U, hs, rhs, i1, j, rt, false);
    LowStar_Regional_regional__uint8_t_
    x0 = { .dummy = NULL, .r_alloc = hash_r_alloc, .r_free = hash_r_free };
    hcpy.copy(rt, mroot);
    *mt
    =
      (
        (merkle_tree){
          .offset = prefix,
          .i = i1,
          .j = j,
          .hs = hs,
          .rhs_ok = true,
          .rhs = rhs,
          .mroot = mroot
        }
      );
  }
}

void path_insert(LowStar_Vector_vector_str__uint8_t_ *p1, uint8_t *hp)
{
  LowStar_Vector_vector_str__uint8_t_ pv = p1[0U];
  LowStar_Vector_vector_str__uint8_t_ ipv = LowStar_Vector_insert__uint8_t_(pv, hp);
  *p1 = ipv;
}

static uint32_t mt_path_length_step(uint32_t k1, uint32_t j, bool actd)
{
  if (j == (uint32_t)0U)
  {
    return (uint32_t)0U;
  }
  else
  {
    if (k1 % (uint32_t)2U == (uint32_t)0U)
    {
      if (j == k1 || (j == k1 + (uint32_t)1U && !actd))
      {
        return (uint32_t)0U;
      }
      else
      {
        return (uint32_t)1U;
      }
    }
    else
    {
      return (uint32_t)1U;
    }
  }
}

static uint32_t mt_path_length(uint32_t lv, uint32_t k1, uint32_t j, bool actd)
{
  if (j == (uint32_t)0U)
  {
    return (uint32_t)0U;
  }
  else
  {
    bool nactd = actd || j % (uint32_t)2U == (uint32_t)1U;
    return
      mt_path_length_step(k1,
        j,
        actd)
      + mt_path_length(lv + (uint32_t)1U, k1 / (uint32_t)2U, j / (uint32_t)2U, nactd);
  }
}

static void
mt_get_path_(
  uint32_t lv,
  LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_ hs,
  LowStar_Vector_vector_str__uint8_t_ rhs,
  uint32_t i1,
  uint32_t j,
  uint32_t k1,
  LowStar_Vector_vector_str__uint8_t_ *p1,
  bool actd
)
{
  uint32_t ofs = offset_of(i1);
  if (!(j == (uint32_t)0U))
  {
    uint32_t ofs1 = offset_of(i1);
    if (k1 % (uint32_t)2U == (uint32_t)1U)
    {
      uint8_t
      *uu____0 =
        LowStar_Vector_index__uint8_t_(LowStar_Vector_index__LowStar_Vector_vector_str_uint8_t_(hs,
            lv),
          k1 - (uint32_t)1U - ofs1);
      LowStar_Vector_vector_str__uint8_t_ pv = p1[0U];
      LowStar_Vector_vector_str__uint8_t_ ipv = LowStar_Vector_insert__uint8_t_(pv, uu____0);
      *p1 = ipv;
    }
    else
    {
      if (!(k1 == j))
      {
        if (k1 + (uint32_t)1U == j)
        {
          if (actd)
          {
            uint8_t *uu____1 = LowStar_Vector_index__uint8_t_(rhs, lv);
            LowStar_Vector_vector_str__uint8_t_ pv = p1[0U];
            LowStar_Vector_vector_str__uint8_t_ ipv = LowStar_Vector_insert__uint8_t_(pv, uu____1);
            *p1 = ipv;
          }
        }
        else
        {
          uint8_t
          *uu____2 =
            LowStar_Vector_index__uint8_t_(LowStar_Vector_index__LowStar_Vector_vector_str_uint8_t_(hs,
                lv),
              k1 + (uint32_t)1U - ofs1);
          LowStar_Vector_vector_str__uint8_t_ pv = p1[0U];
          LowStar_Vector_vector_str__uint8_t_ ipv = LowStar_Vector_insert__uint8_t_(pv, uu____2);
          *p1 = ipv;
        }
      }
    }
    bool ite;
    if (j % (uint32_t)2U == (uint32_t)0U)
    {
      ite = actd;
    }
    else
    {
      ite = true;
    }
    mt_get_path_(lv + (uint32_t)1U,
      hs,
      rhs,
      i1 / (uint32_t)2U,
      j / (uint32_t)2U,
      k1 / (uint32_t)2U,
      p1,
      ite);
  }
}

bool
mt_get_path_pre(
  merkle_tree *mt,
  uint64_t idx,
  LowStar_Vector_vector_str__uint8_t_ *p1,
  uint8_t *root
)
{
  merkle_tree uu____0 = *mt;
  LowStar_Vector_vector_str__uint8_t_ uu____1 = *p1;
  uint64_t diff = idx - uu____0.offset;
  uint32_t idx1 = (uint32_t)diff;
  return
    idx
    >= uu____0.offset
    && idx - uu____0.offset <= offset_range_limit
    && uu____0.i <= idx1 && idx1 < uu____0.j && uu____1.sz == (uint32_t)0U;
}

uint32_t
mt_get_path(
  merkle_tree *mt,
  uint64_t idx,
  LowStar_Vector_vector_str__uint8_t_ *p1,
  uint8_t *root
)
{
  LowStar_Regional_regional__uint8_t_
  x0 = { .dummy = NULL, .r_alloc = hash_r_alloc, .r_free = hash_r_free };
  void (*copy1)(uint8_t *x0, uint8_t *x1) = hcpy.copy;
  mt_get_root(mt, root);
  merkle_tree mtv = *mt;
  uint64_t diff = idx - mtv.offset;
  uint32_t idx1 = (uint32_t)diff;
  uint32_t i1 = mtv.i;
  uint32_t ofs = offset_of(mtv.i);
  uint32_t j = mtv.j;
  LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_ hs = mtv.hs;
  LowStar_Vector_vector_str__uint8_t_ rhs = mtv.rhs;
  uint8_t
  *ih =
    LowStar_Vector_index__uint8_t_(LowStar_Vector_index__LowStar_Vector_vector_str_uint8_t_(hs,
        (uint32_t)0U),
      idx1 - ofs);
  LowStar_Vector_vector_str__uint8_t_ pv = p1[0U];
  LowStar_Vector_vector_str__uint8_t_ ipv = LowStar_Vector_insert__uint8_t_(pv, ih);
  *p1 = ipv;
  mt_get_path_((uint32_t)0U, hs, rhs, i1, j, idx1, p1, false);
  return j;
}

static LowStar_Vector_vector_str__uint8_t_
LowStar_Vector_flush__uint8_t_(
  LowStar_Vector_vector_str__uint8_t_ vec,
  uint8_t *ia,
  uint32_t i
)
{
  uint32_t fsz = vec.sz - i;
  uint32_t asz;
  if (vec.sz == i)
  {
    asz = (uint32_t)1U;
  }
  else
  {
    asz = fsz;
  }
  uint8_t **vs = vec.vs;
  KRML_CHECK_SIZE(sizeof (uint8_t *), asz);
  uint8_t **fvs = KRML_HOST_MALLOC(sizeof (uint8_t *) * asz);
  for (uint32_t _i = 0U; _i < asz; ++_i)
    fvs[_i] = ia;
  memcpy(fvs, vs + i, fsz * sizeof vs[0U]);
  KRML_HOST_FREE(vs);
  return ((LowStar_Vector_vector_str__uint8_t_){ .sz = fsz, .cap = asz, .vs = fvs });
}

static LowStar_Vector_vector_str__uint8_t_
LowStar_RVector_flush__uint8_t_(
  LowStar_Regional_regional__uint8_t_ rg,
  LowStar_Vector_vector_str__uint8_t_ rv,
  uint32_t i
)
{
  if (!(i == (uint32_t)0U))
  {
    LowStar_RVector_free_elems__uint8_t_(rg, rv, i - (uint32_t)1U);
  }
  LowStar_Vector_vector_str__uint8_t_ frv = LowStar_Vector_flush__uint8_t_(rv, rg.dummy, i);
  return frv;
}

static void
mt_flush_to_(
  uint32_t lv,
  LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_ hs,
  uint32_t pi,
  uint32_t i1
)
{
  uint32_t oi = offset_of(i1);
  uint32_t opi = offset_of(pi);
  if (!(oi == opi))
  {
    uint32_t ofs = oi - opi;
    LowStar_Vector_vector_str__uint8_t_
    hvec = LowStar_Vector_index__LowStar_Vector_vector_str_uint8_t_(hs, lv);
    LowStar_Vector_vector_str__uint8_t_
    flushed =
      LowStar_RVector_flush__uint8_t_((
          (LowStar_Regional_regional__uint8_t_){
            .dummy = NULL,
            .r_alloc = hash_r_alloc,
            .r_free = hash_r_free
          }
        ),
        hvec,
        ofs);
    LowStar_RVector_assign__LowStar_Vector_vector_str_uint8_t_((
        (LowStar_Regional_regional__LowStar_Vector_vector_str__uint8_t_){
          .dummy = hash_vec_dummy,
          .r_alloc = hash_vec_r_alloc,
          .r_free = hash_vec_r_free
        }
      ),
      hs,
      lv,
      flushed);
    mt_flush_to_(lv + (uint32_t)1U, hs, pi / (uint32_t)2U, i1 / (uint32_t)2U);
  }
}

bool mt_flush_to_pre(merkle_tree *mt, uint64_t idx)
{
  merkle_tree mtv = *mt;
  uint64_t diff = idx - mtv.offset;
  uint32_t idx1 = (uint32_t)diff;
  return
    idx
    >= mtv.offset
    && idx - mtv.offset <= offset_range_limit
    && idx1 >= mtv.i && idx1 < mtv.j;
}

void mt_flush_to(merkle_tree *mt, uint64_t idx)
{
  merkle_tree mtv = *mt;
  uint64_t offset1 = mtv.offset;
  uint64_t diff = idx - offset1;
  uint32_t idx1 = (uint32_t)diff;
  LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_ hs = mtv.hs;
  mt_flush_to_((uint32_t)0U, hs, mtv.i, idx1);
  *mt
  =
    (
      (merkle_tree){
        .offset = mtv.offset,
        .i = idx1,
        .j = mtv.j,
        .hs = hs,
        .rhs_ok = mtv.rhs_ok,
        .rhs = mtv.rhs,
        .mroot = mtv.mroot
      }
    );
}

bool mt_flush_pre(merkle_tree *mt)
{
  merkle_tree uu____0 = *mt;
  return uu____0.j > uu____0.i;
}

void mt_flush(merkle_tree *mt)
{
  merkle_tree mtv = *mt;
  uint64_t off = mtv.offset;
  uint32_t j = mtv.j;
  uint32_t j1 = j - (uint32_t)1U;
  uint64_t jo = off + (uint64_t)j1;
  mt_flush_to(mt, jo);
}

static void
LowStar_RVector_free_elems_from__uint8_t_(
  LowStar_Regional_regional__uint8_t_ rg0,
  LowStar_Vector_vector_str__uint8_t_ rv0,
  uint32_t idx0
)
{
  LowStar_Regional_regional__uint8_t_ rg = rg0;
  LowStar_Vector_vector_str__uint8_t_ rv = rv0;
  uint32_t idx = idx0;
  while (true)
  {
    uint8_t *uu____0 = LowStar_Vector_index__uint8_t_(rv, idx);
    rg.r_free(uu____0);
    if (idx + (uint32_t)1U < rv.sz)
    {
      idx = idx + (uint32_t)1U;
    }
    else
    {
      return;
    }
  }
  KRML_HOST_PRINTF("KreMLin abort at %s:%d\n%s\n",
    __FILE__,
    __LINE__,
    "unreachable, returns inserted above");
  KRML_HOST_EXIT(255U);
}

static LowStar_Vector_vector_str__uint8_t_
LowStar_Vector_shrink__uint8_t_(LowStar_Vector_vector_str__uint8_t_ vec, uint32_t new_size)
{
  return ((LowStar_Vector_vector_str__uint8_t_){ .sz = new_size, .cap = vec.cap, .vs = vec.vs });
}

static LowStar_Vector_vector_str__uint8_t_
LowStar_RVector_shrink__uint8_t_(
  LowStar_Regional_regional__uint8_t_ rg,
  LowStar_Vector_vector_str__uint8_t_ rv,
  uint32_t new_size
)
{
  uint32_t size = rv.sz;
  if (new_size >= size)
  {
    return rv;
  }
  else
  {
    LowStar_RVector_free_elems_from__uint8_t_(rg, rv, new_size);
    LowStar_Vector_vector_str__uint8_t_ frv = LowStar_Vector_shrink__uint8_t_(rv, new_size);
    return frv;
  }
}

static void
mt_retract_to_(
  LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_ hs,
  uint32_t lv,
  uint32_t i1,
  uint32_t s,
  uint32_t j
)
{
  if (!(lv >= hs.sz))
  {
    LowStar_Vector_vector_str__uint8_t_
    hvec = LowStar_Vector_index__LowStar_Vector_vector_str_uint8_t_(hs, lv);
    uint32_t old_len = j - offset_of(i1);
    uint32_t new_len = s - offset_of(i1);
    LowStar_Vector_vector_str__uint8_t_
    retracted =
      LowStar_RVector_shrink__uint8_t_((
          (LowStar_Regional_regional__uint8_t_){
            .dummy = NULL,
            .r_alloc = hash_r_alloc,
            .r_free = hash_r_free
          }
        ),
        hvec,
        new_len);
    LowStar_RVector_assign__LowStar_Vector_vector_str_uint8_t_((
        (LowStar_Regional_regional__LowStar_Vector_vector_str__uint8_t_){
          .dummy = hash_vec_dummy,
          .r_alloc = hash_vec_r_alloc,
          .r_free = hash_vec_r_free
        }
      ),
      hs,
      lv,
      retracted);
    if (lv + (uint32_t)1U < hs.sz)
    {
      mt_retract_to_(hs, lv + (uint32_t)1U, i1 / (uint32_t)2U, s / (uint32_t)2U, j / (uint32_t)2U);
    }
  }
}

bool mt_retract_to_pre(merkle_tree *mt, uint64_t r)
{
  merkle_tree mtv = *mt;
  uint64_t diff = r - mtv.offset;
  uint32_t r1 = (uint32_t)diff;
  return r >= mtv.offset && r - mtv.offset <= offset_range_limit && mtv.i <= r1 && r1 < mtv.j;
}

void mt_retract_to(merkle_tree *mt, uint64_t r)
{
  merkle_tree mtv = *mt;
  uint64_t offset1 = mtv.offset;
  uint64_t diff = r - offset1;
  uint32_t r1 = (uint32_t)diff;
  LowStar_Vector_vector_str__LowStar_Vector_vector_str__uint8_t_ hs = mtv.hs;
  mt_retract_to_(hs, (uint32_t)0U, mtv.i, r1 + (uint32_t)1U, mtv.j);
  *mt
  =
    (
      (merkle_tree){
        .offset = mtv.offset,
        .i = mtv.i,
        .j = r1 + (uint32_t)1U,
        .hs = hs,
        .rhs_ok = false,
        .rhs = mtv.rhs,
        .mroot = mtv.mroot
      }
    );
}

static void
mt_verify_(
  uint32_t k10,
  uint32_t j0,
  LowStar_Vector_vector_str__uint8_t_ *p10,
  uint32_t ppos0,
  uint8_t *acc0,
  bool actd0
)
{
  uint32_t k1 = k10;
  uint32_t j = j0;
  LowStar_Vector_vector_str__uint8_t_ *p1 = p10;
  uint32_t ppos = ppos0;
  uint8_t *acc = acc0;
  bool actd = actd0;
  while (true)
  {
    if (j == (uint32_t)0U)
    {
      return;
    }
    else
    {
      bool nactd = actd || j % (uint32_t)2U == (uint32_t)1U;
      if (k1 % (uint32_t)2U == (uint32_t)0U)
      {
        if (j == k1 || (j == k1 + (uint32_t)1U && !actd))
        {
          k1 = k1 / (uint32_t)2U;
          j = j / (uint32_t)2U;
          actd = nactd;
        }
        else
        {
          uint8_t *phash = LowStar_Vector_index__uint8_t_(*p1, ppos);
          hash_2(acc, phash, acc);
          k1 = k1 / (uint32_t)2U;
          j = j / (uint32_t)2U;
          ppos = ppos + (uint32_t)1U;
          actd = nactd;
        }
      }
      else
      {
        uint8_t *phash = LowStar_Vector_index__uint8_t_(*p1, ppos);
        hash_2(phash, acc, acc);
        k1 = k1 / (uint32_t)2U;
        j = j / (uint32_t)2U;
        ppos = ppos + (uint32_t)1U;
        actd = nactd;
      }
    }
  }
  KRML_HOST_PRINTF("KreMLin abort at %s:%d\n%s\n",
    __FILE__,
    __LINE__,
    "unreachable, returns inserted above");
  KRML_HOST_EXIT(255U);
}

bool
mt_verify_pre(
  merkle_tree *mt,
  uint64_t k1,
  uint64_t j,
  LowStar_Vector_vector_str__uint8_t_ *p1,
  uint8_t *rt
)
{
  merkle_tree uu____0 = *mt;
  LowStar_Vector_vector_str__uint8_t_ uu____1 = *p1;
  uint64_t diff0 = k1 - uu____0.offset;
  uint32_t k2 = (uint32_t)diff0;
  uint64_t diff = j - uu____0.offset;
  uint32_t j1 = (uint32_t)diff;
  return
    k1
    < j
    && k1 >= uu____0.offset && k1 - uu____0.offset <= offset_range_limit
    && j >= uu____0.offset && j - uu____0.offset <= offset_range_limit
    && uu____1.sz == (uint32_t)1U + mt_path_length((uint32_t)0U, k2, j1, false);
}

static bool buf_eq__uint8_t(uint8_t *b1, uint8_t *b2, uint32_t len1)
{
  if (len1 == (uint32_t)0U)
  {
    return true;
  }
  else
  {
    uint8_t a1 = b1[len1 - (uint32_t)1U];
    uint8_t a2 = b2[len1 - (uint32_t)1U];
    bool teq = buf_eq__uint8_t(b1, b2, len1 - (uint32_t)1U);
    return a1 == a2 && teq;
  }
}

bool
mt_verify(
  merkle_tree *mt,
  uint64_t k1,
  uint64_t j,
  LowStar_Vector_vector_str__uint8_t_ *p1,
  uint8_t *rt
)
{
  merkle_tree mtv = *mt;
  uint64_t diff0 = k1 - mtv.offset;
  uint32_t k2 = (uint32_t)diff0;
  uint64_t diff = j - mtv.offset;
  uint32_t j1 = (uint32_t)diff;
  LowStar_Regional_regional__uint8_t_
  x00 = { .dummy = NULL, .r_alloc = hash_r_alloc, .r_free = hash_r_free };
  uint8_t *ih = x00.r_alloc();
  LowStar_Regional_regional__uint8_t_
  x01 = { .dummy = NULL, .r_alloc = hash_r_alloc, .r_free = hash_r_free };
  void (*copy1)(uint8_t *x0, uint8_t *x1) = hcpy.copy;
  copy1(LowStar_Vector_index__uint8_t_(*p1, (uint32_t)0U), ih);
  mt_verify_(k2, j1, p1, (uint32_t)1U, ih, false);
  bool r = buf_eq__uint8_t(ih, rt, hash_size);
  LowStar_Regional_regional__uint8_t_
  x0 = { .dummy = NULL, .r_alloc = hash_r_alloc, .r_free = hash_r_free };
  x0.r_free(ih);
  return r;
}

